# -*- coding: utf-8 -*-
"""MPMedical_V3.8

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pwFJ3G6eMnufolLJb4DPPxKUqxnKzCEV

###Goals:
* Update the llm.chain method (Deprecated for the time being) - Complete
* Minimize the number of packages used. - Complete
* Use packages mostly from langchain - Complete
* Create a prompt template that can be used to feed both question and answers to the model. - Complete

Source: https://github.com/sophiamyang/tutorials-LangChain/blob/main/LangChain_QA.ipynb

##1. Initialize the LLM & Embedding Model
"""

!pip install --upgrade --quiet  huggingface_hub

!pip install langchain==0.1.6
from langchain_community.llms import HuggingFaceHub
from langchain.chains import LLMChain

import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

llm=HuggingFaceHub(repo_id="openchat/openchat-3.5-0106", model_kwargs={"max_length":8192, "temperature":0.4})

"""##2. Load & Prepare the Documents"""

!pip install PyPDF2
import os
import PyPDF2
from PyPDF2 import PdfReader , PdfWriter, PdfMerger

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

pdfFiles = [] # variable
root_dir = "/content/gdrive/My Drive/Documents/"

for root, dirs, filenames in os.walk(root_dir): # Root and directory pathway.
    for filename in filenames:
            pdfFiles.append(os.path.join(root,filename))
            # Appending files to root name from OS (operating system).

# Sorting the files by forcing everything to lower case.
pdfFiles.sort(key=str.lower)

# Assigning the pdfWriter() function to pdfWriter.
pdfWriter = PyPDF2.PdfWriter()

pdfFiles

for filename in pdfFiles:
    pdfFileObj = open(filename, 'rb')
    print(pdfFileObj)
    pdfReader = PyPDF2.PdfReader(pdfFileObj)
    for pageNum in range(0, len(pdfReader.pages)):
        pageObj = pdfReader.pages[pageNum]
        pdfWriter.add_page(pageObj)

pdfOutput = open('Test_Merge_01.pdf', 'wb')
pdfWriter.write(pdfOutput)
pdfOutput.close()

!pip install pypdf
!pip install sentence_transformers
!pip install pinecone-client
!pip install llama-cpp-python #
!pip install chromadb

from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Pinecone
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from langchain.indexes import VectorstoreIndexCreator
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
import os

"""https://docs.pinecone.io/docs/langchain
- Method for loading pinecone has changed.
"""

#PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'd0133012-5c4a-4aae-b597-20dfb0573977')
#PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')

#PC = Pinecone(
#    api_key=PINECONE_API_KEY,  # find at app.pinecone.io
#    environment=PINECONE_API_ENV  # next to api key in console
#)

loader = PyPDFLoader("/content/Test_Merge_01.pdf")
data = loader.load()
text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs=text_splitter.split_documents(data)

#index = PC.Index('mp1')
model_id = "sentence-transformers/all-MiniLM-L6-v2"
embeddings=HuggingFaceEmbeddings(model_name=model_id)

"""##Updated method for retrieving answer using vectorstore index.
* Note: Most methods used here are for test purposes. They mostly do not work in the favor of our desired model.
"""

index = VectorstoreIndexCreator(
    # split the documents into chunks
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),
    # select which embeddings we want to use
    embedding=embeddings,
    # use Chroma as the vectorestore to index and search embeddings
    vectorstore_cls=Chroma
).from_loaders([loader])
query = "What are the major causes of injury in the military?"
index.query(llm=llm, question=query, chain_type="map_reduce") #Can also use "stuff" for chain type. However, easily exceeds token limit.
#Problem with vectorstore: Spits out information in an "informational" sense. Unreadable at worst.

#QA chain's issue: Uses all of the documents. Easily exceeds the token limit.
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm=llm, chain_type="refine")
query = "What are the most common injuries in the military?"
result = chain.invoke({"input_documents":docs,"question": query})

from langchain.chains import RetrievalQA


# split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)
texts = text_splitter.split_documents(docs)


# create the vectorestore to use as the index
db = Chroma.from_documents(texts, embeddings)
# expose this index in a retriever interface
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k":2})
# create a chain to answer questions
qa = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
query = "What are the most common injuries during training?"
result = qa.invoke({"query": query})
print(result)

"""###Re-integration of Pinecone:"""

!pip install --upgrade --quiet  langchain-pinecone
from langchain_pinecone import Pinecone as PCVS

!pip install pinecone-client

from pinecone import Pinecone

import os
PINECONE_API_KEY = "d0133012-5c4a-4aae-b597-20dfb0573977"
os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY

#Download the embedding model.
model_id = "sentence-transformers/all-MiniLM-L6-v2"

embeddings=HuggingFaceEmbeddings(model_name=model_id)

PCVS = PCVS(
    pinecone_api_key='d0133012-5c4a-4aae-b597-20dfb0573977',  # find at app.pinecone.io
    embedding = embeddings,
    index_name = 'mp1'
)

PC = Pinecone(api_key = 'd0133012-5c4a-4aae-b597-20dfb0573977')
index_name = "mp1"

docsearch = PCVS.from_documents(documents=docs, embedding=embeddings, index_name= index_name)

#QA chain's issue: Uses all of the documents. Easily exceeds the token limit.
from langchain.chains.question_answering import load_qa_chain
chain = load_qa_chain(llm=llm, chain_type="stuff")
query = "What are the most common injuries in the military?"

docs=docsearch.similarity_search(query)

#result = chain.invoke({"input_documents":docs,"question": query})
chain.run(input_documents=docs, question=query)
#Success!
#Keep the langchain version <0.2 so we can keep using the run function (Cheeky)

"""###Prepare Googletrans"""

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

#Translator Function Test
translator = Translator()
translation = translator.translate("하늘은 파랗고 날씨는 따뜻하다", dest='en')
print(translation.text)
#print(translate_text)

"""#Translator Test"""

chain=load_qa_chain(llm, chain_type="stuff")
query=translator.translate("훈련중 가장 많이 다치는 부위는 어딘가요?", dest='en')
docs=docsearch.similarity_search(query.text)
result = chain.run(input_documents=docs, question=query)
print(result)
translated_result = translator.translate(result, dest='ko')
print(translated_result.text)
#Part after ###Explanation is the only information that counts.

string = translated_result.text
say = "설명"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces
print(after)

"""###Implementing prompt templates to 1. structure the question and answers and 2. Automate the Q&A process"""

from langchain.prompts import PromptTemplate
prompt = PromptTemplate.from_template("I am experiencing pain in{injured_area}. I have had this injury for {time}. I had this injury after {cause}. What are some advices you can give to lessen the symptoms?")
#.from_template function simplifies the function.
injured_area = "knees"
time = "2 weeks"
cause = "after a field training where we had to run across the mountain."
prompt = prompt.format(injured_area = injured_area, time = time, cause = cause)


chain=load_qa_chain(llm, chain_type="stuff")
docs=docsearch.similarity_search(prompt)
result = chain.run(input_documents=docs, question=prompt)
print(result)

#Answer: You should consider that the common diagnosis was iliotibial band syndrome (6.2%) and that a significant proportion of the injuries occurred during the first 11 weeks of the programme.
#The longest rehabilitation times were for stress fractures of the femur, calcaneus and tibia. You should also consider that medial tibial stress syndrome had the greatest impact on training, accounting for almost 20% of all days spent in rehabilitation.
#Works!!!

string = "string blah say foo bar"
say = "say"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces
print(after)

"""###Test on MP Soldiers

##일병 김OO
"""

prompt = PromptTemplate.from_template("I am experiencing pain in{injured_area}. I have had this injury for {time}. I had this injury after {cause}. What is the possible diagnosis of this symptom? What are some advices you can give to lessen the symptoms? Give me 4 lines at least")
#.from_template function simplifies the function.
injured_area = translator.translate("정강이 뼈", dest='en')
time = "5 months"
cause = translator.translate("매일 3km씩 달리는 훈련을 거쳤기 때문입니다.", dest='en')
prompt = prompt.format(injured_area = injured_area.text, time = time, cause = cause.text)


chain=load_qa_chain(llm, chain_type="stuff")
docs=docsearch.similarity_search(prompt)
result = chain.run(input_documents=docs, question=prompt)
#print(result)
translated_result = translator.translate(result, dest='ko')
#print(translated_result.text)

string = translated_result.text
say = "유용한 답변"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces
print(after)

"""##일병 변OO"""

prompt = PromptTemplate.from_template("I am experiencing pain in{injured_area}. I have had this injury for {time}. I experience pain when {cause}. What is the possible diagnosis of this symptom? What are some advices you can give to lessen the symptoms? Give me 4 lines at least")
#.from_template function simplifies the function.
injured_area = translator.translate("팔꿈치", dest='en')
time = "3 years"
cause = translator.translate("팔굽혀펴기 훈련을 할때 ", dest='en')
prompt = prompt.format(injured_area = injured_area.text, time = time, cause = cause.text)
print(prompt)


chain=load_qa_chain(llm, chain_type="stuff")
docs=docsearch.similarity_search(prompt)
result = chain.run(input_documents=docs, question=prompt)
#print(result)

string = result
say = "Helpful Answer"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces

translated_result = translator.translate(after, dest='ko')
print(translated_result.text)

"""##일병 허OO"""

prompt = PromptTemplate.from_template("I am experiencing pain in{injured_area}. I have had this injury for {time}. I experience pain when {cause}. What is the possible diagnosis of this symptom? What are some advices you can give to lessen the symptoms? Give me 4 lines at least")
#.from_template function simplifies the function.
injured_area = translator.translate("오른쪽 어깨", dest='en')
time = "5 months"
cause = translator.translate("군장 달리기 훈련과 팔굽혀펴기 훈련을 할때 ", dest='en')
prompt = prompt.format(injured_area = injured_area.text, time = time, cause = cause.text)
print(prompt)


chain=load_qa_chain(llm, chain_type="stuff")
docs=docsearch.similarity_search(prompt)
result = chain.run(input_documents=docs, question=prompt)
#print(result)

string = result
say = "Helpful Answer"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces

translated_result = translator.translate(after, dest='ko')
print(translated_result.text)

"""##상병 김OO"""

prompt = PromptTemplate.from_template("I am experiencing pain in{injured_area}. I have had this injury for {time}. I experience pain when {cause}. What is the possible diagnosis of this symptom? What are some advices you can give to lessen the symptoms? Give me 4 lines at least")
#.from_template function simplifies the function.
injured_area = translator.translate("오른쪽 발목", dest='en')
time = "2 months"
cause = translator.translate("뛰는 훈련과 축구를 한 후 ", dest='en')
prompt = prompt.format(injured_area = injured_area.text, time = time, cause = cause.text)
print(prompt)


chain=load_qa_chain(llm, chain_type="stuff")
docs=docsearch.similarity_search(prompt)
result = chain.run(input_documents=docs, question=prompt)
#print(result)

string = result
say = "Helpful Answer"
after = string[string.index(say) + len(say):] # +1 if you're worried about spaces

translated_result = translator.translate(after, dest='ko')
print(translated_result.text)